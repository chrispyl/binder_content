{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d856f0",
   "metadata": {},
   "source": [
    "# <u><p style=\"text-align: center;\">Introduction to Apache Spark</p></u>\n",
    "\n",
    "### Contents of this notebook\n",
    "* What a Resilient Distributed Dataset (RDD) is\n",
    "* How to load data into Spark\n",
    "* Difference between transformations and actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f428b8",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef7db9",
   "metadata": {},
   "source": [
    "Apache Spark is a system that allows us to process large amounts of data in parallel. A core concept of Spark that allows to scale data processing operations is the *resilient distributed dataset* (**RDD**). RDDs are immutable collections of elements which can be partitioned across several computers and be operated in parallel. \n",
    "\n",
    "When we read data with Spark, they are translated into RDDs. In those RDDs we can then apply `map` and `reduce` operations which Spark automatically executes in parallel.\n",
    "\n",
    "In this notebook we are going to see how to initialize Spark, how to create RDDs, and how to work with RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a152b",
   "metadata": {},
   "source": [
    "#### Spark initialization\n",
    "To work with Spark we need a Spark cluster and a way to access to it. In the following cell we configure a cluster and create a `SparkSession` to access it. There is code in this cell that you may not understand, but this part is not the focus of our course. These are just steps that we have to perform to configure our cluster. Note that this code will output some red lines with warnings which should not worry you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e8a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#'swan_spark_conf' is a configuration provided by a plugin for Jupyter. We further extend this configuration with proxy settings.\n",
    "swan_spark_conf = swan_spark_conf.setAll([('spark.ui.proxyBase', os.environ['JUPYTERHUB_SERVICE_PREFIX'] + 'proxy/4040')])\n",
    "\n",
    "#instantiate a SparkSession object with our configuration\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .config(conf=swan_spark_conf)\\\n",
    "            .appName('Spark introduction')\\\n",
    "            .getOrCreate()\n",
    "\n",
    "#get a SparkContext object which will allow us to work with RDDs\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#set Spark log level\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64181776",
   "metadata": {},
   "source": [
    "`SparkSession` is the entry point of Spark, it provides access to Spark functionality. `SparkContext` represents a connection to a cluster. It can be used to create RDDs and distribute data on that cluster. When starting this and the rest of the notebooks of this course, we are going to initialize Spark by running this cell.\n",
    "\n",
    "Note that this cell may take some time to be executed since Apache Spark is launching in the background. An asterisk on the left of the cell means that execution has not finished yet, while a number denotes that execution has already finished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67ab6b6",
   "metadata": {},
   "source": [
    "#### Creating an RDD\n",
    "\n",
    "All parallel work in Spark is done on RDDs, so the first thing we need to do is to\n",
    "convert our data to an RDD. To do this we are going to use the `parallelize`\n",
    "method on our `SparkContext` `sc`. \n",
    "\n",
    "`parallelize` takes two arguments: (1) our data, and optionally (2) a number of partitions to split the data. Below we create an RDD with two partitions from a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = ['Dog', 'Cat', 'Rabbit', 'Hare', 'Deer', 'Gull', 'Woodpecker', 'Mole']\n",
    "animals_rdd = sc.parallelize(animals, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6502338",
   "metadata": {},
   "source": [
    "Notice the difference on the data type of the list and the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef02be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the type of animals is: ' + str(type(animals)))\n",
    "print('the type of animals_rdd is: ' + str(type(animals_rdd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494733cd",
   "metadata": {},
   "source": [
    "Also, observe below how the elements of `animals_rdd` are distributed into partitions. You can see that in the number of Tasks that have been executed for this job. Each sub-list represents a partition and its elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b295187",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(animals_rdd.glom().collect()) #2 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a814d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "animals_rdd = sc.parallelize(animals, 3) #3 partitions\n",
    "print(animals_rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd802848",
   "metadata": {},
   "outputs": [],
   "source": [
    "animals_rdd = sc.parallelize(animals, 4) #4 partitions\n",
    "print(animals_rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e731f07b",
   "metadata": {},
   "source": [
    "The number of partitions affects the processing performance as it represents the number of 'pieces' of data that a cluster can work with in parallel. If we have too many partitions, not all of them will be processed in parallel because we might not have enough computing nodes. On the other hand, if we have too few partitions, some computing nodes may be left unused.\n",
    "\n",
    "The number of partitions is a parameter that requires calibration for intensive tasks but for our notebooks we are going to always have two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de6c197",
   "metadata": {},
   "source": [
    "#### Lazy evaluation\n",
    "\n",
    "Now let's suppose that we have the following RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf4b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = ['Dog', 'Dog', 'Dog', 'Cat', 'Cat', 'Parrot']\n",
    "duplicates_rdd = sc.parallelize(duplicates, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f278c2",
   "metadata": {},
   "source": [
    "and we want to find its distinct elements using the `distinct` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51093e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_elements = duplicates_rdd.distinct()\n",
    "print(distinct_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a20540a",
   "metadata": {},
   "source": [
    "As we observe, printing `distinct_elements` does not print any values. That's because of an evaluation strategy called **lazy evaluation** that Spark follows.\n",
    "\n",
    "In **lazy evaluation** parts of our code are executed only when there is a need to do so. The benefits of **lazy evaluation** are:\n",
    "* Saving time by executing operations only when we ask for a result to be produced\n",
    "* Similarly saving system resources\n",
    "* 'Automatic' performance improvements through operation planning since we know which of them we have to perform before we ask for results\n",
    "\n",
    "Spark achieves this by having two distinct types of operations, **transformations** and **actions**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc9d21",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "Transformations are operations that will not be completed when the code in a cell is executed - they will only get executed once an **action** is called. We can think of tranformations as operations that we know how to do, but we will not do until there is a reason for it. An example of a transformation might be to map a function over an RDD, or to filter a set of values.\n",
    "\n",
    "### Actions\n",
    "\n",
    "Actions are commands that are computed by Spark when the corresponding code is executed in a cell. They consist of running all of the previous transformations in order to get back an actual result. An action is composed of one or more *jobs*, and each job consist of *tasks*. Tasks are executed in parallel when possible.\n",
    "\n",
    "Below are some examples of transformations and actions:\n",
    "\n",
    "<img src=\"images/transformations_actions.png\" width=\"350\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a0480b",
   "metadata": {},
   "source": [
    "### Code examples\n",
    "\n",
    "In the following examples we are going to use Spark to create RDDs and apply transformations and actions to them. \n",
    "\n",
    "***Example 1:*** revisits the conversion of Celcius temperatures to Fahrenheit using `map` with RDDs.  \n",
    "***Example 2:*** calculates the average temperature of a lake using `reduce` with RDDs.   \n",
    "\n",
    "When executing cells that contain Spark actions an interface will appear which shows the progress of the operations. Also, more details like operation planning for each example can be found in the [Spark UI]()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed1e13c",
   "metadata": {},
   "source": [
    "#### Example 1: Celcius to Fahrenheit\n",
    "\n",
    "For our first example we are going to revisit the conversion of temperatures from Fahrenheit to Celcius degrees. The function that we used for the conversion previously was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c5986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_Fahrenheit(temperature):\n",
    "    return temperature * 9/5 + 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e7ec09",
   "metadata": {},
   "source": [
    "Here, we are going to do the same conversion but in a scalable way (by utilizing Spark). Our list of temperatures is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c553f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "celcius_temperatures = [10, 15, 9, -2, 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6815e8c5",
   "metadata": {},
   "source": [
    "We first pass our data to Spark by converting the temperature list to an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea69aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "celcius_temperatures_rdd = sc.parallelize(celcius_temperatures, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3b1d63",
   "metadata": {},
   "source": [
    "Then we verify that the rdd contains the temperatures using `collect`. `collect` returns the contents of an RDD as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e215ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(celcius_temperatures_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf63f25",
   "metadata": {},
   "source": [
    "Our next step is to map the function `to_Fahrenheit` over the rdd. The syntax of `map` in Spark is:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac0ffb44",
   "metadata": {},
   "source": [
    "rdd.map(function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec0a74b",
   "metadata": {},
   "source": [
    "In our case this translates to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7017ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "celcius_temperatures_rdd.map(to_Fahrenheit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ecb52",
   "metadata": {},
   "source": [
    "However, `map` is a transformation, so we won't see any results until we use an action. For this reason we use `collect`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = celcius_temperatures_rdd.map(to_Fahrenheit)\n",
    "print(temperatures.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11772c0e",
   "metadata": {},
   "source": [
    "#### Example 2: Temperature average\n",
    "\n",
    "Now let's see how we could find the average water temperature of lake Como, Italy, in July. First we convert our data to an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b78c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_temperatures = [23.4, 27.5, 25.1, 22.1, 23.9]\n",
    "water_temperatures_rdd = sc.parallelize(water_temperatures, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0afa33",
   "metadata": {},
   "source": [
    "Then, we define the addition function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcaed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(number_1, number_2):\n",
    "    return number_1 + number_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5818cfdf",
   "metadata": {},
   "source": [
    "and after that we can add the temperatures using `reduce`. The syntax of `reduce` is below:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "933b167e",
   "metadata": {},
   "source": [
    "rdd.reduce(function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a27c875",
   "metadata": {},
   "source": [
    "So it becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c25947",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_temperatures = water_temperatures_rdd.reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6510c",
   "metadata": {},
   "source": [
    "Next, to calculate the average temperature we need to know how many temperatures we have. To count them we can use `count`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_temperatures = water_temperatures_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38d585",
   "metadata": {},
   "source": [
    "And now we are ready to calculate the average water temperature in lake Como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48265da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_temperature = added_temperatures / number_of_temperatures\n",
    "print(average_temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f032dc0",
   "metadata": {},
   "source": [
    "<span style=\"display:none\" id=\"question1\">W3sicXVlc3Rpb24iOiAiV2hlbiB3ZSBsb2FkIGRhdGEgdG8gU3BhcmsgdGhleSBhcmUgY29udmVydGVkIHRvIFJERHM6IiwgInR5cGUiOiAibXVsdGlwbGVfY2hvaWNlIiwgImFuc3dlcnMiOiBbeyJjb2RlIjogIlRydWUiLCAiY29ycmVjdCI6IHRydWV9LCB7ImNvZGUiOiAiRmFsc2UiLCAiY29ycmVjdCI6IGZhbHNlfV19XQ==</span>\n",
    "\n",
    "<span style=\"display:none\" id=\"question2\">W3sicXVlc3Rpb24iOiAiJ01hcCcgYW5kICdSZWR1Y2UnIG9wZXJhdGlvbnMgaGFwcGVuIGluIHBhcmFsbGVsIGluIFJERHM6IiwgInR5cGUiOiAibXVsdGlwbGVfY2hvaWNlIiwgImFuc3dlcnMiOiBbeyJjb2RlIjogIlRydWUiLCAiY29ycmVjdCI6IHRydWV9LCB7ImNvZGUiOiAiRmFsc2UiLCAiY29ycmVjdCI6IGZhbHNlfV19XQ==</span>\n",
    "\n",
    "<span style=\"display:none\" id=\"question3\">W3sicXVlc3Rpb24iOiAiU3BhcmsgdHJhbnNmb3JtYXRpb25zIGFyZSBleGVjdXRlZCB3aGVuIHRoZSBjb3JyZXNwb25kaW5nIGNlbGwgaXMgZXhlY3V0ZWQ6IiwgInR5cGUiOiAibXVsdGlwbGVfY2hvaWNlIiwgImFuc3dlcnMiOiBbeyJjb2RlIjogIlRydWUiLCAiY29ycmVjdCI6IGZhbHNlLCAiZmVlZGJhY2siOiAiVGhhdCdzIHdoYXQgYWN0aW9ucyBkby4ifSwgeyJjb2RlIjogIkZhbHNlIiwgImNvcnJlY3QiOiB0cnVlfV19XQ==</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abee451b",
   "metadata": {},
   "source": [
    "### Practice questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08757a4",
   "metadata": {},
   "source": [
    "#### Q1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00670340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "\n",
    "display_quiz('#question1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0216d8b0",
   "metadata": {},
   "source": [
    "#### Q2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c8f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz('#question2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea1794",
   "metadata": {},
   "source": [
    "#### Q3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b41bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz('#question3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb7d33",
   "metadata": {},
   "source": [
    "### More advanced examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538be163",
   "metadata": {},
   "source": [
    "##### Example A1: Operation chaining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c43cdbf",
   "metadata": {},
   "source": [
    "There are cases where we need to perform multiple operations to an RDD. Suppose that we want to find how many distinct even numbers exist in the following list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c106f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [34, 1, 12, 71, 92, 5, 6, 23, 11, 45]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70727370",
   "metadata": {},
   "source": [
    "A possible solution is to define the function which checks if a number is even:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be972934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_even(number):\n",
    "    return number%2==0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e4f6a2",
   "metadata": {},
   "source": [
    "convert the data to an RDD:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca04a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rdd = sc.parallelize(data, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ac9df",
   "metadata": {},
   "source": [
    "filter out odd numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381fae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rdd = data_rdd.filter(is_even)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66f39a",
   "metadata": {},
   "source": [
    "remove duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9db6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rdd = data_rdd.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6529485",
   "metadata": {},
   "source": [
    "and finally count the remaining numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be05bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13339c2",
   "metadata": {},
   "source": [
    "These operations can summarized below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8753d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rdd = sc.parallelize(data, 2)\n",
    "data_rdd = data_rdd.filter(is_even)\n",
    "data_rdd = data_rdd.distinct()\n",
    "result = data_rdd.count()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff72d172",
   "metadata": {},
   "source": [
    "We notice that as our operations increase in number, our code becomes verbose and difficult to comprehend. For this reason Spark allows to *chain* operations. With chaining, the code of the previous cell could be rewritten as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f081b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sc.parallelize(data, 2)\\\n",
    "            .filter(is_even)\\\n",
    "            .distinct()\\\n",
    "            .count()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd60f57",
   "metadata": {},
   "source": [
    "and now the code is more readable, and it is more clear to see the flow of the operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e15c5",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "* [RDDs](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds)\n",
    "* [Lazy evaluation](https://en.wikipedia.org/wiki/Lazy_evaluation)\n",
    "* [RDD transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)\n",
    "* [RDD actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)\n",
    "* [Map with RDDs](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html?highlight=map#pyspark.RDD.map)\n",
    "* [Reduce with RDDs](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.reduce.html?highlight=reduce#pyspark.RDD.reduce)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
