{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6e3ee4",
   "metadata": {},
   "source": [
    "# <u><p style=\"text-align: center;\">Dataframes</p></u>\n",
    "\n",
    "### Contents of this notebook\n",
    "* What a Spark DataFrame is\n",
    "* How to load data into DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985be4ab",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "As we have seen, RDDs are the building blocks of Spark. RDDs have several advantages but in some cases their use can be problematic. Such cases can occur because Spark does not optimize transformations when we perform them directly to RDDs. Another example is that working with RDDs in some programming languages (including Python) can lead to poor performance. Also, transformation chains with RDDs can be difficult to comprehend since they show how the result will be achieved but not what the result will be.\n",
    "\n",
    "Spark **DataFrames** were conceived to overcome the aforementioned problems. Similar to RDDs, DataFrames are distributed collections of data. The difference is that DataFrames provide a high-level abstraction over RDDs that allows us to use a query language to manipulate data. This abstraction is a logical plan that represents data and a schema. The logical plan is converted to a physical plan for execution. This conversion brings us closer to **what** we want to do rather than **how** we have to do it, because we let Spark figure out the most efficient way to carry out the operations. DataFrames are generally faster than RDDs, and they perform the same no matter what programming language we use with Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a070831",
   "metadata": {},
   "source": [
    "### Code examples\n",
    "\n",
    "In the following examples we are going to use different transformations and action on dataframes. \n",
    "\n",
    "***Example 1:*** creates a DataFrame and performs ordering operations.   \n",
    "***Example 2:*** loads data into a DataFrame from a file and performs aggregations.   \n",
    "***Example 3:*** queries a DataFrame using SQL syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d436a67",
   "metadata": {},
   "source": [
    "Before proceeding to the examples, we are going to initialize Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#'swan_spark_conf' is a configuration provided by a plugin for Jupyter. We further extend this configuration with proxy settings.\n",
    "swan_spark_conf = swan_spark_conf.setAll([('spark.ui.proxyBase', os.environ['JUPYTERHUB_SERVICE_PREFIX'] + 'proxy/4040')])\n",
    "\n",
    "#instantiate a SparkSession object with our configuration\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .config(conf=swan_spark_conf)\\\n",
    "            .appName('Spark DataFrames')\\\n",
    "            .getOrCreate()\n",
    "\n",
    "#set Spark log level\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3e13c0",
   "metadata": {},
   "source": [
    "#### Example 1: Creating a DataFrame\n",
    "\n",
    "In our first example we are going to create a DataFrame 'manually' which will contain the data of our cows. The data consist of the name, breed and weight of each cows. From those data we would like to have an overwview of the weight and population of each breed.\n",
    "\n",
    "So, first we create our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b4081",
   "metadata": {},
   "outputs": [],
   "source": [
    "cowsDF = spark.createDataFrame([(\"Joel\", \"Angus\", 450), \n",
    "                               (\"Marcia\", \"Belted Galloway\", 320),\n",
    "                               (\"Gregor\", \"Hereford\", 390),\n",
    "                               (\"Anne\", \"Angus\", 400),\n",
    "                               (\"Ravi\", \"Belted Galloway\", 250),\n",
    "                               (\"Marcia\", \"Belted Galloway\", 320)],\n",
    "                              (\"Name\", \"Breed\", \"Weight\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f371d40",
   "metadata": {},
   "source": [
    "and examine it using the function `show`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cowsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c821c",
   "metadata": {},
   "source": [
    "We notice that 'Marcia' has been entered twice in our records so we have to clean our data before we proceed. We can delete duplicate records with the `dropDuplicates` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dfd29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cowsDF = cowsDF.dropDuplicates([\"Name\", \"Breed\", \"Weight\"])\n",
    "cowsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e6bab",
   "metadata": {},
   "source": [
    "Next, we would like to inspect the weight of our cows from lighter to heavier. To do this we order our DataFrame using the `orderBy` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderedDF = cowsDF.orderBy(\"Weight\")\n",
    "orderedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da930fcf",
   "metadata": {},
   "source": [
    "Now that we have an overview of the weight, we would like to order the weights based on breed. This can be done by combining `groupBy` with `orderBy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b915973",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedDF = cowsDF.orderBy(['Breed','Weight'])\n",
    "groupedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5453a900",
   "metadata": {},
   "source": [
    "Finally, we would like to count how many cows of each breed we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f077e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "countDF = cowsDF.groupBy(\"Breed\").count()\n",
    "countDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241af17b",
   "metadata": {},
   "source": [
    "#### Example 2: *.csv* to DataFrame\n",
    "\n",
    "DataFrames provide a convenient way to work with tabular data. In this example, we are going to read a file with Spark and convert into a DataFrame. The file contains the minimum and maximum daily temperatures for the years 2010-2015 in De Bilt, Netherlands. \n",
    "\n",
    "Then, we are going to find the minimum and maximum temperatures that occured during these years and also count how many days the temperature was below 0 $^\\text{o}C$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376fc7b",
   "metadata": {},
   "source": [
    "So, the first step is to load the data into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba926b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF = spark.read.csv(\"/home/jovyan/datasets/knmi-debilt.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca4357c",
   "metadata": {},
   "source": [
    "and then examine how the data look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d380b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeba9be",
   "metadata": {},
   "source": [
    "Dates are formatted as YYYYMMDD, temperatures are in Celcius degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4888f",
   "metadata": {},
   "source": [
    "Next, to find the minimum and maximum temperatures we are going to use **aggregations** over the DataFrame. We can perform aggregations by using the `agg` function. The parameters of `agg` are expressions that indicate the aggregation that we want to perform. To find the maximum temperature a possible solution is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3928f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "result = dataDF.agg(F.max(\"Tmax\")) #notice that Tmax is the name of the column\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732b990",
   "metadata": {},
   "source": [
    "and similarly for the minimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dataDF.agg(F.min(\"Tmin\")) #notice that Tmin is the name of the column\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed1311",
   "metadata": {},
   "source": [
    "Now, to find how many days the temperature was below 0 $^\\text{o}C$, we are first going to keep only the days with the required temperature by using the `filter` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "below_zeroDF = dataDF.filter(F.col(\"Tmin\") < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2433a7",
   "metadata": {},
   "source": [
    "followed by the `count` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "below_zeroDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3bdf1c",
   "metadata": {},
   "source": [
    "#### Example 3: Performing SQL queries\n",
    "\n",
    "When working with DataFrames we can also write SQL queries against the DataFrame. Using the previous dataset we are going to preserve only the rows of the year 2012. To do this we are first creating a temporary view of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2bfdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.createOrReplaceTempView(\"data_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a118cc",
   "metadata": {},
   "source": [
    "and then query it using sql syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a2a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_2012DF = spark.sql(\"SELECT Date, Tmin, Tmax FROM data_view WHERE SUBSTRING(Date,1,4) == 2012\")\n",
    "only_2012DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cfc99",
   "metadata": {},
   "source": [
    "<span style=\"display:none\" id=\"question1\">W3sicXVlc3Rpb24iOiAiU3BhcmsgRGF0YUZyYW1lIG9wZXJhdGlvbnMgYXJlIG9wdGltaXplZCBieSBTcGFyay4iLCAidHlwZSI6ICJtdWx0aXBsZV9jaG9pY2UiLCAiYW5zd2VycyI6IFt7ImNvZGUiOiAiVHJ1ZSIsICJjb3JyZWN0IjogdHJ1ZX0sIHsiY29kZSI6ICJGYWxzZSIsICJjb3JyZWN0IjogZmFsc2V9XX1d</span>\n",
    "\n",
    "<span style=\"display:none\" id=\"question2\">W3sicXVlc3Rpb24iOiAiU3BhcmsgRGF0YUZyYW1lcyBhcmUgYnVpbHQgb24gdG9wIG9mIFJERHMuIiwgInR5cGUiOiAibXVsdGlwbGVfY2hvaWNlIiwgImFuc3dlcnMiOiBbeyJjb2RlIjogIlRydWUiLCAiY29ycmVjdCI6IHRydWV9LCB7ImNvZGUiOiAiRmFsc2UiLCAiY29ycmVjdCI6IGZhbHNlfV19XQ==</span>\n",
    "\n",
    "<span style=\"display:none\" id=\"question3\">W3sicXVlc3Rpb24iOiAiQ2hvb3NlIHRoZSBjb3JyZWN0IGFuc3dlcnM6IiwgInR5cGUiOiAibXVsdGlwbGVfY2hvaWNlIiwgImFuc3dlcnMiOiBbeyJjb2RlIjogIlNwYXJrIERhdGFGcmFtZXMgYXJlICAgIG5vbi1kaXN0cmlidXRlZCBjb2xsZS0gICBjdGlvbnMgb2YgZGF0YS4iLCAiY29ycmVjdCI6IGZhbHNlLCAiZmVlZGJhY2siOiAiVGhleSBhcmUgZGlzdHJpYnV0ZWQuIn0sIHsiY29kZSI6ICJXZSBjYW4gdXNlIFNRTCBxdWVyaWVzICBkaXJlY3RseSB3aXRoIERhdGFGcmEtICAgbWVzLiIsICJjb3JyZWN0IjogdHJ1ZX0sIHsiY29kZSI6ICJUaGUgcGVyZm9ybWFuY2Ugd2UgZ2V0ICB3aGVuIHVzaW5nIERhdGFGcmFtZXMgaXMgcHJvZ3JhbW1pbmcgbGFuZ3VhZ2UgICAgZGVwZW5kZW50LiIsICJjb3JyZWN0IjogZmFsc2UsICJmZWVkYmFjayI6ICJEYXRhRnJhbWVzIGhhdmUgdGhlIHNhbWUgcGVyZm9ybWFuY2UgcmVnYXJkbGVzcyBvZiB0aGUgbGFuZ3VhZ2UgdXNlZC4ifSwgeyJjb2RlIjogIldoZW4gd29ya2luZyB3aXRoICAgICAgIERhdGFGcmFtZXMgd2UgaGF2ZSB0byAgICAgICAgIGNhcmVmdWxseSB0aGluayB0aGUgICAgIG9yZGVyIG9mIHRoZSBvcGVyYXRpb25zICAgdGhhdCAgd2Ugd2FudCB0byBhcHBseS4iLCAiY29ycmVjdCI6IHRydWV9XX1d</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf30966",
   "metadata": {},
   "source": [
    "### Practice questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1726cd5",
   "metadata": {},
   "source": [
    "#### Q1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def45251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "\n",
    "display_quiz(\"#question1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7afd0f",
   "metadata": {},
   "source": [
    "#### Q2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f19fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(\"#question2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7311414e",
   "metadata": {},
   "source": [
    "#### Q3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4619255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(\"#question3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26b31d",
   "metadata": {},
   "source": [
    "### More advanced examples:\n",
    "\n",
    "In this section we are going to see an example on how to improve execution speed when working with DataFrames. This example is not essential to understand the rest of the course. Although, we would recommend you try it as it will help you deepen your knowledge on this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c351aaba",
   "metadata": {},
   "source": [
    "#### Example A1: Caching for speed\n",
    "\n",
    "Similarly to RDDs, DataFrame operations are divided into transformations and actions. Results will not be computed until we call an action. When we call an action all the previous transformations up to this action are performed and we get a result. However, during this procedure no intermmediate state of the DataFrame is stored. So, if we need one of these states, Spark has to start the computations from the beggining.\n",
    "\n",
    "Such an example is depicted below. Each circle represents a transformation that a DataFrame has to go through:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf76c5c",
   "metadata": {},
   "source": [
    "<img src=\"images/transformation_chain.png\" alt=\"drawing\" width=\"170\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b313fb",
   "metadata": {},
   "source": [
    "After transformation B, the transformation chain splits into two branches. We can calculate the chain A->B->C but to calculate A->B->D Spark has to start from A again, as the state B is not stored. For computationally expensive transformations we might not have the time or resources to recompute the same operations. \n",
    "\n",
    "For this reason, we can store an intermmediate state using the `cache` function. Spark can then start computations from this state instead of the beggining of the transformation chain.\n",
    "\n",
    "Below is an example showcasing how the `cache` function can be used. We have data from potato fields in the Netherlands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70237957",
   "metadata": {},
   "outputs": [],
   "source": [
    "potatoesDF = spark.read.csv(\"/home/jovyan/datasets/potatoes.csv\", header=True, inferSchema=True)\n",
    "potatoesDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313e817c",
   "metadata": {},
   "source": [
    "as well as soil consistency data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45897435",
   "metadata": {},
   "outputs": [],
   "source": [
    "soilDF = spark.read.csv(\"/home/jovyan/datasets/soil.csv\", header=True, inferSchema=True)\n",
    "soilDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cf7944",
   "metadata": {},
   "source": [
    "and we want to prepare the dataset for some data analysis tasks. To prepare the dataset we would like to perform the following operations:\n",
    "* create a planting year column\n",
    "* match the soil properties to the potato fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98e7c4",
   "metadata": {},
   "source": [
    "To add the year column in `potatoesDF` we can use the `withColumn` function. Here we also use the `year` and `to_timestamp` functions to process the date format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b46d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "potatoesDF = potatoesDF.withColumn(\"Year\", F.year(F.to_timestamp('PlantingDate', 'dd-MM-yy')))\n",
    "\n",
    "potatoesDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474d12d4",
   "metadata": {},
   "source": [
    "Next, to match the soil properties to the potato fields we can use the `join` function, to join `potatoesDF` with `soilDF`  on *SoilType* column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedDF = potatoesDF.join(soilDF, \"SoilType\")\n",
    "joinedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c28d1a9",
   "metadata": {},
   "source": [
    "Our dataset is now ready, we let Spark know the transformations that we want to perform and we are going to proceed with our data analysis tasks. If we compare our transformations with the image of this example, adding the year column corresponds to transformation A, and matching the soil properties corresponds to transformation B.\n",
    "\n",
    "Moving forwards, for our data analysis we have two tasks. The first, which corresponds to transformation C in the image, is to find the average amount of fertilizer applied for each clay percentage in our dataset. The second, which corresponds to transformation D, is to find the the average amount of fertilizer applied to each cultivar in 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5bb089",
   "metadata": {},
   "source": [
    "The first task can be achieved by first grouping based on the *ClayPercentage* column and then aggregating on *FertilizerApplied* column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90e4216",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1DF = joinedDF.groupBy(\"ClayPercentage\")\\\n",
    "                    .agg(F.mean(F.col(\"FertilizerApplied\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8c2b2c",
   "metadata": {},
   "source": [
    "The second task can be achieved by filtering based on the *Year* column, grouping on *Cultivar* column and aggregating on *FertilizerApplied*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b6ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task2DF = joinedDF.filter(F.col(\"Year\") == 2020)\\\n",
    "                    .groupBy(\"Cultivar\")\\\n",
    "                    .agg(F.mean(F.col(\"FertilizerApplied\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1091a",
   "metadata": {},
   "source": [
    "Notice here that for both tasks we started working on top of `joinedDF`, so both tasks have the same state as root. \n",
    "\n",
    "Until now we haven't perform any actions to get results. So let's apply an action (`show`) to get the results of our first task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed51b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326aaa8",
   "metadata": {},
   "source": [
    "Task 1 took quite some time to finish. During this time Spark started from the beggining of the transformation  chain (adding the year column), added the soil properties and then performed the task. So `joinedDF` has been created here.\n",
    "\n",
    "Now let's get the results of task 2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11129c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "task2DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e32a52",
   "metadata": {},
   "source": [
    "We notice here that this task also takes quite some time to finish. As we observe on the Spark jobs panel, Spark started computing the transformation chain from the beggining even if we had already computed `joinedDF` for task 1.\n",
    "\n",
    "Now, we are going to tell Spark to store `joinedDF` when it is computed, so that the result can be available for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f3b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfeaf5f",
   "metadata": {},
   "source": [
    "Next, we are going to perform again task 1, to force the computation of `joinedDF`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd28f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "task1DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf539c58",
   "metadata": {},
   "source": [
    "Task 1 took as much time as it did before but check what happens when we perform task 2 which requires `joinedDF` to be available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73317e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "task2DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343cc264",
   "metadata": {},
   "source": [
    "For task 2 the computation time was reduced since it already found `joinedDF` in memory. In this way, when we know that our operations have common ancestor states we can speed up our computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c0d44",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "* [What is a DataFrame in Spark](https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes)\n",
    "* [DataFrame documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html?highlight=dataframe#pyspark.sql.DataFrame)\n",
    "* [Functions to manipulate DataFrames](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions)\n",
    "* [SQL syntax for DataFrames](https://spark.apache.org/docs/latest/api/sql/index.html)\n",
    "* [Caching DataFrames](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.cache.html?highlight=cache#pyspark.sql.DataFrame.cache)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
