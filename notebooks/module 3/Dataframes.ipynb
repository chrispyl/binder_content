{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6e3ee4",
   "metadata": {},
   "source": [
    "# <u><p style=\"text-align: center;\">Dataframes</p></u>\n",
    "\n",
    "### Learning goals  \n",
    "Students will:  \n",
    "* Learn about Spark Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985be4ab",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "As we have seen, RDDs are the building blocks of Spark. RDDs have several advantages but in some cases their use can be problematic. Such cases can occur because Spark does not optimize transformations when we perform them directly to RDDs. Another example is that working with RDDs in some programming languages (including Python) can lead to poor performance. Also, transformation chains with RDDs can be difficult to comprehend since they show how the result will be achieved but not what the result will be.\n",
    "\n",
    "Spark **DataFrames** were conceived to overcome the aforementioned problems. Similar to RDDs, DataFrames are distributed collections of data. The difference is that DataFrames provide a high-level abstraction over RDDs that allows us to use a query language to manipulate data. This abstraction is a logical plan that represents data and a schema. The logical plan is converted to a physical plan for execution. This conversion brings us closer to **what** we want to do rather than **how** we have to do it, because we let Spark figure out the most efficient way to carry out the operations. Dataframes are generally faster than RDDs, and they perform the same no matter what programming language we use with Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a070831",
   "metadata": {},
   "source": [
    "### Code examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d436a67",
   "metadata": {},
   "source": [
    "Before proceeding to the examples, we are going to initialize Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import os\n",
    "\n",
    "#'swan_spark_conf' is a configuration provided by a plugin for Jupyter. We further extend this configuration with proxy settings.\n",
    "swan_spark_conf = swan_spark_conf.setAll([('spark.ui.proxyBase', os.environ['JUPYTERHUB_SERVICE_PREFIX'] + 'proxy/4040')])\n",
    "\n",
    "#instantiate a SparkContext object with our configuration\n",
    "sc = SparkContext.getOrCreate(conf=swan_spark_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e074c",
   "metadata": {},
   "source": [
    "and create an SQLContext which will help us create DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0c4527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3e13c0",
   "metadata": {},
   "source": [
    "#### Example 1:\n",
    "\n",
    "In our first example we are going to create a DataFrame 'manually' which will contain the data of our cows. The data consist of the name, breed and weight of each cows. From those data we would like to have an overwview of the weight and population of each breed.\n",
    "\n",
    "So, first we create our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b4081",
   "metadata": {},
   "outputs": [],
   "source": [
    "cowsDF = sqlc.createDataFrame([(\"Joel\", \"Angus\", 450), \n",
    "                               (\"Marcia\", \"Belted Galloway\", 320),\n",
    "                               (\"Gregor\", \"Hereford\", 390),\n",
    "                               (\"Anne\", \"Angus\", 400),\n",
    "                               (\"Ravi\", \"Belted Galloway\", 250),\n",
    "                               (\"Marcia\", \"Belted Galloway\", 320)],\n",
    "                              (\"Name\", \"Breed\", \"Weight\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f371d40",
   "metadata": {},
   "source": [
    "and examine it using the function `show`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cowsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c821c",
   "metadata": {},
   "source": [
    "We notice that 'Marcia' has been entered twice in our records so we have to clean our data before we proceed. We can delete duplicate records with the `dropDuplicates` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dfd29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cowsDF = cowsDF.dropDuplicates([\"Name\", \"Breed\", \"Weight\"])\n",
    "cowsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e6bab",
   "metadata": {},
   "source": [
    "Next, we would like to inspect the weight of our cows from lighter to heavier. To do this we order our DataFrame using the `orderBy` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderedDF = cowsDF.orderBy(\"Weight\")\n",
    "orderedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da930fcf",
   "metadata": {},
   "source": [
    "Now that we have an overview of the weight, we would like to order the weights based on breed. This can be done by combining `groupBy` with `orderBy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b915973",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedDF = cowsDF.orderBy(['Breed','Weight'])\n",
    "groupedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5453a900",
   "metadata": {},
   "source": [
    "Finally, we would like to count how many cows of each breed we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f077e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "countDF = cows.groupBy(\"Breed\").count()\n",
    "countDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241af17b",
   "metadata": {},
   "source": [
    "#### Example 2:\n",
    "\n",
    "DataFrames provide a convenient way to work with tabular data. In this example, we are going to read a file with Spark and convert into a DataFrame. The file contains the minimum and maximum daily temperatures for the years 2010-2015 in De Bilt, Netherlands. \n",
    "\n",
    "Then, we are going to find the minimum and maximum temperatures that occured during these years and also count how many days the temperature was below 0 $^\\text{o}C$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c376fc7b",
   "metadata": {},
   "source": [
    "So, the first step is to load the data into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba926b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF = sqlc.read.csv(\"/home/jovyan/datasets/knmi-debilt.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca4357c",
   "metadata": {},
   "source": [
    "and then examine how the data look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d380b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeba9be",
   "metadata": {},
   "source": [
    "Dates are formatted as YYYYMMDD, temperatures are in Celcius degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4888f",
   "metadata": {},
   "source": [
    "Next, to find the minimum and maximum temperatures we are going to use **aggregations** over the DataFrame. We can perform aggregations by using the `agg` function. The parameters of `agg` are expressions that indicate the aggregation that we want to perform. To find the maximum temperature a possible solution is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3928f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "result = dataDF.agg(F.max(\"Tmax\")) #notice that Tmax is the name of the column\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732b990",
   "metadata": {},
   "source": [
    "and similarly for the minimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dataDF.agg(F.min(\"Tmin\")) #notice that Tmin is the name of the column\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed1311",
   "metadata": {},
   "source": [
    "Now, to find how many days the temperature was below 0 $^\\text{o}C$, we are first going to keep only the days with the required temperature by using the `filter` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "below_zeroDF = dataDF.filter(F.col(\"Tmin\") < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2433a7",
   "metadata": {},
   "source": [
    "followed by the `count` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "below_zeroDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3bdf1c",
   "metadata": {},
   "source": [
    "#### Example 3:\n",
    "\n",
    "When working with DataFrames we can also write SQL queries against the DataFrame. Using the previous dataset we are going to preserve only the rows of the year 2012. To do this we are first creating a temporary view of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2bfdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.createOrReplaceTempView(\"data_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a118cc",
   "metadata": {},
   "source": [
    "and then query it using sql syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a2a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_2012DF = sqlc.sql(\"SELECT Date, Tmin, Tmax FROM data_view WHERE SUBSTRING(Date,1,4) == 2012\")\n",
    "only_2012DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cfc99",
   "metadata": {},
   "source": [
    "<span style=\"display:none\" id=\"question1\">W3sicXVlc3Rpb24iOiAiU3BhcmsgRGF0YUZyYW1lIG9wZXJhdGlvbnMgYXJlIG9wdGltaXplZCBieSBTcGFyay4iLCAidHlwZSI6ICJtdWx0aXBsZV9jaG9pY2UiLCAiYW5zd2VycyI6IFt7ImNvZGUiOiAiVHJ1ZSIsICJjb3JyZWN0IjogdHJ1ZX0sIHsiY29kZSI6ICJGYWxzZSIsICJjb3JyZWN0IjogZmFsc2V9XX1d</span>\n",
    "\n",
    "<span style=\"display:none\" id=\"question2\">W3sicXVlc3Rpb24iOiAiU3BhcmsgRGF0YUZyYW1lcyBhcmUgYnVpbHQgb24gdG9wIG9mIFJERHMuIiwgInR5cGUiOiAibXVsdGlwbGVfY2hvaWNlIiwgImFuc3dlcnMiOiBbeyJjb2RlIjogIlRydWUiLCAiY29ycmVjdCI6IHRydWV9LCB7ImNvZGUiOiAiRmFsc2UiLCAiY29ycmVjdCI6IGZhbHNlfV19XQ==</span>\n",
    "\n",
    "<span style=\"display:none\" id=\"question3\">W3sicXVlc3Rpb24iOiAiQ2hvb3NlIHRoZSBjb3JyZWN0IGFuc3dlcnM6IiwgInR5cGUiOiAibXVsdGlwbGVfY2hvaWNlIiwgImFuc3dlcnMiOiBbeyJjb2RlIjogIlNwYXJrIERhdGFGcmFtZXMgYXJlICAgIG5vbi1kaXN0cmlidXRlZCBjb2xsZWN0aW9ucyBvZiBkYXRhLiIsICJjb3JyZWN0IjogZmFsc2UsICJmZWVkYmFjayI6ICJUaGV5IGFyZSBkaXN0cmlidXRlZC4ifSwgeyJjb2RlIjogIldlIGNhbiB1c2UgU1FMIHF1ZXJpZXMgIGRpcmVjdGx5IHdpdGggRGF0YUZyYW1lcy4iLCAiY29ycmVjdCI6IHRydWV9LCB7ImNvZGUiOiAiVGhlIHBlcmZvcm1hbmNlIHdlIGdldCAgd2hlbiB1c2luZyBEYXRhRnJhbWVzIGlzIHByb2dyYW1taW5nIGxhbmd1YWdlIGRlcGVuZGVudC4iLCAiY29ycmVjdCI6IGZhbHNlLCAiZmVlZGJhY2siOiAiRGF0YUZyYW1lcyBoYXZlIHRoZSBzYW1lIHBlcmZvcm1hbmNlIHJlZ2FyZGxlc3Mgb2YgdGhlIGxhbmd1YWdlIHVzZWQuIn0sIHsiY29kZSI6ICJXaGVuIHdvcmtpbmcgd2l0aCAgICAgICBEYXRhRnJhbWVzIHdlIGhhdmUgdG8gY2FyZWZ1bGx5IHRoaW5rIHRoZSBvcmRlciBvZiB0aGUgb3BlcmF0aW9ucyB0aGF0IHdlIHdhbnQgdG8gYXBwbHkuIiwgImNvcnJlY3QiOiB0cnVlfV19XQ==</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf30966",
   "metadata": {},
   "source": [
    "### Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1726cd5",
   "metadata": {},
   "source": [
    "#### Q1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def45251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterquiz import display_quiz\n",
    "\n",
    "display_quiz(\"#question1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7afd0f",
   "metadata": {},
   "source": [
    "#### Q2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f19fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(\"#question2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7311414e",
   "metadata": {},
   "source": [
    "#### Q3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4619255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(\"#question3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26b31d",
   "metadata": {},
   "source": [
    "### More advanced examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c351aaba",
   "metadata": {},
   "source": [
    "#### Example A1:\n",
    "\n",
    "Similarly to RDDs, DataFrame operations are divided into transformations and actions. Results will not be computed until we call an action. When we call an action all the previous transformations up to this action are performed and we get a result. However, during this procedure no intermmediate state of the DataFrame is stored. So, if we need one of these states, Spark has to start the computations from the beggining.\n",
    "\n",
    "Such an example is depicted below. Each circle represents a transformation that a DataFrame has to go through:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5c271",
   "metadata": {},
   "source": [
    "<img src=\"transformation_chain.png\" alt=\"drawing\" width=\"170\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a18a3",
   "metadata": {},
   "source": [
    "After transformation B, the transformation chain splits into two branches. We can calculate the chain A->B->C but to calculate A->B->D Spark has to start from A again, as the state B is not stored. For computationally expensive transformations we might not have the time or resources to recompute the same operations. \n",
    "\n",
    "For this reason, we can store an intermmediate state using the `cache` function. Spark can then start computations from this state instead of the beggining of the transformation chain.\n",
    "\n",
    "Below is an example showcasing the difference that `cache` makes in computation time. We have data from potato fields in the Netherlands as well as soil consistency data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70237957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#potato data\n",
    "potatoes = [(\"Field1\", \"Miranda\", \"15-4-2020\", 120, \"type1\"),\n",
    "            (\"Field2\", \"Miranda\", \"15-4-2020\", 150, \"type1\"),\n",
    "            (\"Field3\", \"Miranda\", \"15-4-2020\", 90, \"type1\"),\n",
    "            (\"Field4\", \"Bintje\", \"15-4-2020\", 100, \"type2\"),\n",
    "            (\"Field5\", \"Bintje\", \"15-4-2020\", 120, \"type2\"),\n",
    "            (\"Field6\", \"Felsina\", \"15-4-2020\", 110, \"type3\"),\n",
    "            (\"Field7\", \"Felsina\", \"15-4-2020\", 135, \"type3\"),\n",
    "            (\"Field8\", \"Felsina\", \"15-4-2020\", 130, \"type3\"),\n",
    "            (\"Field9\", \"Felsina\", \"15-4-2020\", 145, \"type3\"),\n",
    "            (\"Field10\", \"Fontane\", \"15-4-2020\", 110, \"type4\")]\n",
    "\n",
    "#soil data\n",
    "soil = [(\"type1\", 15, 28, 57), \n",
    "        (\"type2\", 4, 15, 81), \n",
    "        (\"type3\", 22, 32, 46), \n",
    "        (\"type4\", 19, 40, 41), \n",
    "        (\"type5\", 9, 11, 80), \n",
    "        (\"type6\", 9, 17, 74)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb759b63",
   "metadata": {},
   "source": [
    "and we want to..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69345546",
   "metadata": {},
   "source": [
    "First we convert the data to DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "potatoesDF = sqlc.createDataFrame(potatoes, (\"Field\", \"Cultivar\", \"PlantingDate\"))\n",
    "\n",
    "soilDF = sqlc.createDataFrame(soil, (\"ClayPercentage\", \"SiltPercentage\", \"SandPercentage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d0b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db6c0d44",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "* https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "* https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html?highlight=dataframe#pyspark.sql.DataFrame\n",
    "* https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions\n",
    "* https://spark.apache.org/docs/latest/api/sql/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed097bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1=[{\n",
    "        \"question\": \"Spark DataFrame operations are optimized by Spark.\",\n",
    "        \"type\": \"multiple_choice\",\n",
    "        \"answers\": [\n",
    "            {\n",
    "                \"code\": \"True\",\n",
    "                \"correct\": True\n",
    "            },\n",
    "            {\n",
    "                \"code\": \"False\",\n",
    "                \"correct\": False\n",
    "            }\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "q2=[{\n",
    "        \"question\": \"Spark DataFrames are built on top of RDDs.\",\n",
    "        \"type\": \"multiple_choice\",\n",
    "        \"answers\": [\n",
    "            {\n",
    "                \"code\": \"True\",\n",
    "                \"correct\": True\n",
    "            },\n",
    "            {\n",
    "                \"code\": \"False\",\n",
    "                \"correct\": False\n",
    "            }\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "q3=[{\n",
    "        \"question\": \"Choose the correct answers:\",\n",
    "        \"type\": \"multiple_choice\",\n",
    "        \"answers\": [\n",
    "            {\n",
    "                \"code\": \"Spark DataFrames are    non-distributed collections of data.\",\n",
    "                \"correct\": False,\n",
    "                \"feedback\": \"They are distributed.\"\n",
    "            },\n",
    "            {\n",
    "                \"code\": \"We can use SQL queries  directly with DataFrames.\",\n",
    "                \"correct\": True\n",
    "            },\n",
    "            {\n",
    "                \"code\": \"The performance we get  when using DataFrames is programming language dependent.\",\n",
    "                \"correct\": False,\n",
    "                \"feedback\": \"DataFrames have the same performance regardless of the language used.\"\n",
    "            },\n",
    "            {\n",
    "                \"code\": \"When working with       DataFrames we have to carefully think the order of the operations that we want to apply.\",\n",
    "                \"correct\": True\n",
    "            }\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "from base64 import b64encode\n",
    "import json\n",
    "print(b64encode(bytes(json.dumps(q1), 'utf8')))\n",
    "print(b64encode(bytes(json.dumps(q2), 'utf8')))\n",
    "print(b64encode(bytes(json.dumps(q3), 'utf8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52cfba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
