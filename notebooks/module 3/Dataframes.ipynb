{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6e3ee4",
   "metadata": {},
   "source": [
    "# <u><p style=\"text-align: center;\">Dataframes</p></u>\n",
    "\n",
    "### Learning goals  \n",
    "Students will:  \n",
    "* Learn about Spark Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985be4ab",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "As we have seen, RDDs are the building blocks of Spark. RDDs have several advantages but in some cases their use can be problematic. Such cases can occur because Spark does not optimize transformations when we perform them directly to RDDs. Another example is that working with RDDs in some programming languages (including Python) can lead to poor performance. Also, transformation chains with RDDs can be difficult to comprehend since they show how the result will be achieved but not what the result will be.\n",
    "\n",
    "Spark **DataFrames** were conceived to overcome the aforementioned problems. Similar to RDDs, DataFrames are distributed collections of data. The difference is that DataFrames provide a high-level abstraction over RDDs that allows us to use a query language to manipulate data. This abstraction is a logical plan that represents data and a schema. The logical plan is converted to a physical plan for execution. This conversion brings us closer to **what** we want to do rather than **how** we have to do it, because we let Spark figure out the most efficient way to carry out the operations. Dataframes are generally faster than RDDs, and they perform the same no matter what programming language we use with Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a070831",
   "metadata": {},
   "source": [
    "### Code examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d436a67",
   "metadata": {},
   "source": [
    "Before proceeding to the examples, we are going to initialize Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import os\n",
    "\n",
    "#'swan_spark_conf' is a configuration provided by a plugin for Jupyter. We further extend this configuration with proxy settings.\n",
    "swan_spark_conf = swan_spark_conf.setAll([('spark.ui.proxyBase', os.environ['JUPYTERHUB_SERVICE_PREFIX'] + 'proxy/4040')])\n",
    "\n",
    "#instantiate a SparkContext object with our configuration\n",
    "sc = SparkContext.getOrCreate(conf=swan_spark_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3e13c0",
   "metadata": {},
   "source": [
    "#### Example 1:\n",
    "\n",
    "DataFram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b4081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "241af17b",
   "metadata": {},
   "source": [
    "#### Example 2:\n",
    "\n",
    "DataFrames provide a convenient way to work with tabular data. In this example, we are going to read a file with Spark and convert into a DataFrame. The file contains the minimum and maximum daily temperatures for the years 2010-2015 in De Bilt, Netherlands. \n",
    "\n",
    "Then, we are going to find the minimum and maximum temperatures that occured during these years, the date when they happened, and also count how many days the temperature was below 0 $^\\text{o}C$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5624f1c7",
   "metadata": {},
   "source": [
    "So, the first step is to load the data into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba926b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF = sc.read.csv('de_bilt.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0008dfe9",
   "metadata": {},
   "source": [
    "and then we examine how the data look like by using the function `show`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ffed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ca0fa",
   "metadata": {},
   "source": [
    "Dates are formatted as YYYYMMDD, temperatures are in Celcius degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e93c9bb",
   "metadata": {},
   "source": [
    "Next, to find the minimum and maximum temperatures we are going to use **aggregations** over the DataFrame. We can perform aggregations by using the `agg` function. The parameters of `agg` are expressions that indicate the aggregation that we want to perform. To find the maximum temperature a possible solution is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a16dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "dataDF.agg(F.max(\"Tmax\")).show() #notice that Tmax is the name of the column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e828aa",
   "metadata": {},
   "source": [
    "and similarly for the minimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17180930",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF.agg(F.min(\"Tmin\")).show() #notice that Tmin is the name of the column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54103394",
   "metadata": {},
   "source": [
    "Now, to find how many days the temperature was below 0 $^\\text{o}C$ we are going to use the `filter` function with an appropriate conditional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ff90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "below_zeroDF = dataDF.filter(F.col(\"Tmin\") < 0)\n",
    "below_zeroDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e8e758",
   "metadata": {},
   "source": [
    "#### Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc4fe05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0216dc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baf30966",
   "metadata": {},
   "source": [
    "### Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff87d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db6c0d44",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed097bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
