{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b4d1a0",
   "metadata": {},
   "source": [
    "# <u><p style=\"text-align: center;\">Extract, Transform, Load (ETL)</p></u>\n",
    "\n",
    "### Contents of this notebook\n",
    "* What an ETL procedure is\n",
    "* ETL case study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebe74eb",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b66a0b8",
   "metadata": {},
   "source": [
    "#### ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e7ea0",
   "metadata": {},
   "source": [
    "The ***Extract, Transform, Load*** procedure refers to copying data from one or more sources into a destination system which represents the data differently from the source(s). The three actions have the following meanings:\n",
    "* **Extract**: retrieve data from a source\n",
    "* **Transform**: convert retrieved data according to rules and lookup tables or create combinations of data from different sources \n",
    "* **Load**: store the data in a different location\n",
    "\n",
    "The ETL procedure is becoming more and more important because we need to handle ever increasing datasets, varying data structures, as well as heterogeneous and multimodal data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33659d",
   "metadata": {},
   "source": [
    "#### Case study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7a06d9",
   "metadata": {},
   "source": [
    "In this notebook, we will examine the possibility of using a data lake to perform an ETL procedure for transforming and storing sensor data. The selected study was originally embedded into the Big Data project of [BREED4FOOD](https://breed4food.com/). \n",
    "\n",
    "![image](https://www.breed4food.com/images/logo.jpg)\n",
    "\n",
    "The case study accomodated an experiment in which the gait score of 200 turkeys was determined. Traditionally, gait scoring is performed by a trained person. In the study, different types of sensors were used to explore if they can describe the gait score recorded by a trained person. For this notebook, we are going to use only the **Force Plate** sensor.\n",
    "\n",
    "During the animal study different data types were acquired by each sensor. For the force plate, these were binary files, called Technical Data Management Streaming (TDMS) files. This file format was created to help engineers and scientists to properly store the large amounts of data generated during simulations and tests. \n",
    "\n",
    "Here, as a first step we are going to showcase an ETL procedure for a single force plate file. Then, we will scale up our ETL procedure to minimize its execution time when a large number of animals are being investigated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ed4b04",
   "metadata": {},
   "source": [
    "### Simple ETL for the forceplate data\n",
    "\n",
    "Before we are able to transform the data it is necessary to load important packages and libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f27ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from nptdms import TdmsFile as td\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "#'swan_spark_conf' is a configuration provided by a plugin for Jupyter. We further extend this configuration with proxy settings.\n",
    "swan_spark_conf = swan_spark_conf.setAll([('spark.ui.proxyBase', os.environ['JUPYTERHUB_SERVICE_PREFIX'] + 'proxy/4040')])\n",
    "\n",
    "#instantiate a SparkSession object with our configuration\n",
    "spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .config(conf=swan_spark_conf)\\\n",
    "            .appName('ETL')\\\n",
    "            .getOrCreate()\n",
    "\n",
    "#set Spark log level\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468b095",
   "metadata": {},
   "source": [
    "#### Extracting the force plate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09642c21",
   "metadata": {},
   "source": [
    "Data coming from the forceplate sensor are stored in a proprietary format that Spark cannot read directly. Thus, we need to extract the data and store them in an open fromat, like CSV.\n",
    "\n",
    "Forceplate data are stored in files with the extention *.tdms*. under a folder named `forceplate_files`. Initially, we generate a list containing the files in the appropriate folder with the extention *.tdms*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac7544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob(os.path.join('/home/jovyan/datasets/forceplate_files', '**/*.tdms'))\n",
    "\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec2c25e",
   "metadata": {},
   "source": [
    "#### Transforming and loading the data\n",
    "\n",
    "Then, we iterate through the list and transform the native data to *.csv* format. Finally, we store them in another folder, named `forceplate_csv_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30356120",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    \n",
    "    print('Source path:', path)\n",
    "    df = td(path).as_dataframe() #transforming the data into a dataframe\n",
    "    \n",
    "    destination_path = '/home/jovyan/datasets/forceplate_csv_files/' + os.path.basename(os.path.dirname(path) + '.csv')\n",
    "    print('Destination path:', destination_path)\n",
    "    df.to_csv(destination_path) #loading (storing) the data to a different location as csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac33fb1",
   "metadata": {},
   "source": [
    "#### Reading one of the *.csv* files\n",
    "\n",
    "If you check the *.csv* files, you will notice that the column names are way too long. Thus, we are going to shorten them by manually defining a schema into which Spark will load them. \n",
    "\n",
    "In the schema declaration below, we define the column names, and the corresponding data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b4008",
   "metadata": {},
   "outputs": [],
   "source": [
    "forceplate_schema = StructType([\n",
    "    StructField('Time', IntegerType()),\n",
    "    StructField('Channel1', DoubleType()),\n",
    "    StructField('Channel2', DoubleType()),\n",
    "    StructField('Channel3', DoubleType()),\n",
    "    StructField('Channel4', DoubleType()),\n",
    "    StructField('Channel5', DoubleType()),\n",
    "    StructField('Channel6', DoubleType()),\n",
    "    StructField('Channel7', DoubleType()),\n",
    "    StructField('Channel8', DoubleType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3252d02",
   "metadata": {},
   "source": [
    "Now, let's read a *.csv* file, with the schema we set-up earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a6a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "channelsDF = spark.read.csv('/home/jovyan/datasets/forceplate_csv_files/18936.csv', header=True, schema=forceplate_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4acd54",
   "metadata": {},
   "source": [
    "and then inspect the dataframe and its schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "channelsDF.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e892e6b",
   "metadata": {},
   "source": [
    "We can use the `count` action to see how many data points are in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a410f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "channelsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bc631b",
   "metadata": {},
   "source": [
    "and also calculate summary statistics from the dataframe, using the `describe` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "channelsDF.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e562b",
   "metadata": {},
   "source": [
    "### Scaling by loading multiple files at once\n",
    "\n",
    "The steps for pre-processing data from a single force plate were explained previously. Here, we will try to scale up by processing several files at once. To load one *.csv*  file we used the statement below:\n",
    "\n",
    "`channelsDF = spark.read.csv('/home/jovyan/datasets/forceplate_csv_files/18936.csv', header=True, schema=forceplate_schema)`\n",
    "\n",
    "Now, we need to load all the files that are stored in the `forceplate_csv_files` folder. Since we are working with Spark, the only thing we have to change is to replace the *.csv* filename with the folder path of the *.csv* files, and all the CSV files will be read into a single dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62895c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "channelsDFall = spark.read.csv('/home/jovyan/datasets/forceplate_csv_files', header=True, schema=forceplate_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee81bbb",
   "metadata": {},
   "source": [
    "Now, we can check if all files were loaded, for example by printing the total amount of records in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a024e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "channelsDFall.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9043a2e9",
   "metadata": {},
   "source": [
    "which is correct, so the code to load all data is working. Note that this is a massively parallelizable operation that would have worked the same with really big data files stored in a remote location.\n",
    "\n",
    "For any further processing of the turkeys individually, we would need a way to identify them. The turkey IDs are contained in the filenames as numbers. For example, the file in `/home/jovyan/datasets/forceplate_csv_files/18936.csv` refers to turkey with ID `18936`. However, you may have noticed that except for their name the files do not include an identifier of the turkey they correspond to. Without identifiers our dataframe is impractical for future analysis. This is a common problem in ETL tasks, but Spark has functionality to solve it. For each row in the dataframe, Spark can recall from which file it was loaded, and we can extract the turkey ID information from its path location.\n",
    "\n",
    "We can do this by using the `withColumn` and `input_file_name` functions, by appending a new column named *input* which contains the *filename* from which each row has been read. The `input_file_name` function returns the full path of the file from which the row has been read, or an empty string if not available. In our case, it contains the turkey identifier (ID). Check the code below and inspect the results of the transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826143c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "channelsDFall = channelsDFall.withColumn('input', F.input_file_name())\n",
    "\n",
    "channelsDFall.select('input').show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2695f13",
   "metadata": {},
   "source": [
    "Now, we added a column with the path of the file from which the the row has been read. Our next step is to split the ID of each turkey from the path.\n",
    "\n",
    "To seperate the ID of each turkey we need to split the input string `file:///home/jovyan/datasets/forceplate_csv_files/18936.csv` by /, and keep the part with the turkey identifier. This can be done with the `split` function. From the resulting array we pick the ninth element, which contains the turkey ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c033051",
   "metadata": {},
   "outputs": [],
   "source": [
    "channelsDFall = channelsDFall.withColumn('ID', F.split(F.col('input'), '/')[7])\n",
    "channelsDFall.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce00ea9",
   "metadata": {},
   "source": [
    "Now, you may have noticed that the file extension (.csv), is still there. To remove it, we can use the `substr` function. With `substr` we can retain the first five digits of the splitted element by extracting a substring. Try the code below and check the output in your ID column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b890911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "channelsDFall = channelsDFall.withColumn('ID', F.split(F.col('input'), '/')[7].substr(0,5))\n",
    "channelsDFall.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091bbb82",
   "metadata": {},
   "source": [
    "At this point, we have extracted all the necessary information for further processing. As a final step, we are going to store the transformed data in *.csv* format in a folder named `transformed_forceplate_csv_files`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85d7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "channelsDFall.write.csv('/home/jovyan/datasets/transformed_forceplate_csv_files/data.csv', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e08753",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "\n",
    "More information about the case study can be found [here](https://www.sciencedirect.com/science/article/pii/S175173112000155X?via%3Dihub). Also, you can find more material on its [code](https://github.com/ionathan/datalake-locomotion) and [open data](https://zenodo.org/record/3563513) repositories. Other links related to the notebook are below:\n",
    "* [Data lakes](https://en.wikipedia.org/wiki/Data_lake)\n",
    "* [ETL procedures](https://en.wikipedia.org/wiki/Extract,_transform,_load)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
