{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccdd3453",
   "metadata": {},
   "source": [
    "### Spark UI address setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f57db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sparkUI=os.environ['JUPYTERHUB_SERVICE_PREFIX'] + 'proxy/4040/jobs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df068cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#for dummy notebook\n",
    "with open('dummy.ipynb', 'r+') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    for cell, cell_num in zip(data['cells'], range(len(data['cells']))):\n",
    "        if cell['id'] == \"ad9d5538\":\n",
    "            data['cells'][cell_num]['source'][2]=\"Click this link [Spark UI](\" + str(sparkUI) + \").\\n\"\n",
    "    \n",
    "    f.seek(0)        # move the cursor back to the beginning of the file then start writing\n",
    "    json.dump(data, f)\n",
    "    f.truncate()     # deal with the case where the new data is smaller than the previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5401494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for introduction to spark notebook\n",
    "\n",
    "with open('/home/jovyan/notebooks/module 4/Introduction to Apache Spark.ipynb', 'r+') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    for cell, cell_num in zip(data['cells'], range(len(data['cells']))):\n",
    "        if cell['id'] == \"06a0480b\":\n",
    "            data['cells'][cell_num]['source'][4]=\n",
    "\"### Code examples\\n\\nIn the following examples we are going to use Spark to create RDDs and apply transformations and actions to them. \\n\\n***Example 1:*** revisits the conversion of Celcius temperatures to Fahrenheit using `map` with RDDs.  \\n***Example 2:*** calculates the average temperature of a lake using `reduce` with RDDs.   \\n\\nWhen executing cells that contain Spark actions an interface will appear which shows the progress of the operations. Also, more details like operation planning for each example can be found in the [Spark UI](\" + str(sparkUI) + \").\\n\",\n",
    "    \n",
    "    f.seek(0)        # move the cursor back to the beginning of the file then start writing\n",
    "    json.dump(data, f)\n",
    "    f.truncate()     # deal with the case where the new data is smaller than the previous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2be74",
   "metadata": {},
   "source": [
    "### Create csv folder for force plate date for ETL notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d16ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('/home/jovyan/datasets/forceplate_csv_files/')\n",
    "os.makedirs('/home/jovyan/datasets/transformed_forceplate_csv_files/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
